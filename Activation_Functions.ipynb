{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vTSvsCrnQw4b"
      },
      "outputs": [],
      "source": [
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Function Converts its input value to a value only in the range of 0 to 1."
      ],
      "metadata": {
        "id": "bxikNK7PZ3Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+math.exp(-x))"
      ],
      "metadata": {
        "id": "-aovqVAnQ2Y2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanh function is similar to sigmoid function but here it returns either 0 or 1.\n"
      ],
      "metadata": {
        "id": "Oazr5asCaHfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tanh(x):\n",
        "  return (math.exp(x)-math.exp(-x)/math.exp(x)+math.exp(-x))"
      ],
      "metadata": {
        "id": "_m9Au_tWaGfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main issue with these both functions are the derivatives at plane slopes in their graph becomes 0, which makes the learning process of the model slow, this whole problem is known as Vanishing Gradients."
      ],
      "metadata": {
        "id": "9RvuXHbNaD4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, to eradicate this issue here comes a new function known as ReLU - Rectified Linear Unit\n",
        "If the input value is less than 0 it returns 0 else it returns the same value as it is."
      ],
      "metadata": {
        "id": "A0FTX_hRarTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(x):\n",
        "  return max(0,x)"
      ],
      "metadata": {
        "id": "PzwOPN_VQ8e1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is similar to ReLU but some what leaky it is."
      ],
      "metadata": {
        "id": "1MAjGZD1bHct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LeakyReLU(x):\n",
        "  return max(0.1*x,x)"
      ],
      "metadata": {
        "id": "Iemq6VQwRCrK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In General for hidden layers we mostly probably prefer activation function as ReLU and in output layer we can use all other functions such as step,sigmoid,tanh."
      ],
      "metadata": {
        "id": "b85FvrtIbUFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def execute(x):\n",
        "  print('Sigmoid Function Output: ',sigmoid(x))\n",
        "  print('ReLU Function Output: ',ReLU(x))\n",
        "  print('Leaky ReLU Function Output: ',LeakyReLU(x))\n",
        "  print('Tanh Function Output: ',tanh(x))"
      ],
      "metadata": {
        "id": "QcCPPwtNRYeH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=int(input('''Enter a Number to see all function's output  '''))\n",
        "execute(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aff5V9kMR4Rk",
        "outputId": "4f2dcf4c-c182-4df5-b8a2-d8629ef95e2b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a Number to see all function's output  67\n",
            "Sigmoid Function Output:  1.0\n",
            "ReLU Function Output:  67\n",
            "Leaky ReLU Function Output:  67\n",
            "Tanh Function Output:  1.2523631708422137e+29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmCfKft1SRS4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}